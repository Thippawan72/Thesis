{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MsIwZL9GB6DnXojABJF5N2V1zIPRxx3d",
      "authorship_tag": "ABX9TyPbkp2jUZaL4rvpUJ442Y5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thippawan72/Thesis/blob/main/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VhBw2AmK4sGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "jEVHz8jCFOnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install moviepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m23WOv5CmOHg",
        "outputId": "0a2c2298-2d39-4403-d31b-09e9a1bd81e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.34.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f05dnQR43St",
        "outputId": "34d56bee-62af-46ad-fd3e-9fed6837bb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "e4GtPYPDnl3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input and output directories\n",
        "formal_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/วิดีโอทั้งหมดกึ่งทางการ'\n",
        "formal_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/Video_กึ่งทางการ'\n",
        "\n",
        "casual_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /วิดีโอทั้งหมดการใช้ชีวิตประจำวัน'\n",
        "casual_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /Video_การใช้ชีวิตประจำวัน'"
      ],
      "metadata": {
        "id": "dxA0Dw77n1ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert GIF to MP4\n",
        "def convert_gif_to_mp4(input_gif, output_mp4):\n",
        "    try:\n",
        "        # Load the GIF file\n",
        "        clip = VideoFileClip(input_gif)\n",
        "\n",
        "        # Write the video to MP4 format\n",
        "        clip.write_videofile(output_mp4, codec='libx264')\n",
        "        print(f\"Successfully converted {input_gif} to {output_mp4}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {input_gif}: {e}\" )"
      ],
      "metadata": {
        "id": "1OSXPthpm8r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all GIFs in a directory\n",
        "#def preprocess_videos(input_dir, output_dir):\n",
        "#   if not os.path.exists(output_dir):\n",
        " #       os.makedirs(output_dir)\n",
        "\n",
        "#    # Loop over all files in the input directory\n",
        " #   for filename in os.listdir(input_dir):\n",
        "  #      if filename.endswith(\".gif\"):\n",
        "   #         input_path = os.path.join(input_dir, filename)\n",
        "    #        output_filename = filename.replace(\".gif\", \".mp4\")\n",
        "     #       output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            # Convert GIF to MP4\n",
        "      #      convert_gif_to_mp4(input_path, output_path)\n",
        "\n",
        "# Start preprocessing the GIFs\n",
        "#preprocess_videos(formal_input_directory, formal_output_directory)"
      ],
      "metadata": {
        "id": "HVpZAsKSna4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5cVDAWIgp7"
      },
      "source": [
        "# PyThaiNLP Get Started\n",
        "\n",
        "Code examples for basic functions in PyThaiNLP https://github.com/PyThaiNLP/pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HsfhZlwInqs"
      },
      "outputs": [],
      "source": [
        "# # pip install required modules\n",
        "# # uncomment if running from colab\n",
        "# # see list of modules in `requirements` and `extras`\n",
        "# # in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
        "\n",
        "#!pip install pythainlp\n",
        "#!pip install epitran"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install required modules"
      ],
      "metadata": {
        "id": "BaT_g8fV-7xp",
        "outputId": "4ac0c43c-953c-4340-eae5-0d919e626a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting required\n",
            "  Downloading required-0.4.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting modules\n",
            "  Downloading modules-1.0.0.tar.gz (525 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from required) (1.16.0)\n",
            "Collecting lark-parser (from required)\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading required-0.4.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: modules\n",
            "  Building wheel for modules (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for modules: filename=modules-1.0.0-py3-none-any.whl size=1198 sha256=0f53e899add5a4c543e99396091cb8c4a16d8d378a6884eefa22101c1067d22b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/1b/5a/0e7760d483cf2ac6001c9df78809f16feb9632607248e3ab78\n",
            "Successfully built modules\n",
            "Installing collected packages: modules, lark-parser, required\n",
            "Successfully installed lark-parser-0.12.0 modules-1.0.0 required-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install epitran"
      ],
      "metadata": {
        "id": "E32blbWe_CLX",
        "outputId": "f6e228dd-0a74-4e13-df5e-715e66174559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.8.30)\n",
            "Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.4\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.5.15)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n",
            "Collecting jamo (from epitran)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting g2pk (from epitran)\n",
            "  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n",
            "Collecting konlpy (from g2pk->epitran)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting python-mecab-ko (from g2pk->epitran)\n",
            "  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n",
            "Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n",
            "Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=0597002713b9e4b55cfafa7f7a8086798ccd88df69448c9c9ee75d453d0656e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n",
            "Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR6Klwc-UAH"
      },
      "source": [
        "## Import PyThaiNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp"
      ],
      "metadata": {
        "id": "9IhnIXQJ-oDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7CkITTf-UAH",
        "outputId": "258fb3c7-e243-43e5-bb0a-799a633917ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5.0.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6gy4MLGIgp9"
      },
      "source": [
        "## Thai Characters\n",
        "\n",
        "PyThaiNLP provides some ready-to-use Thai character set (e.g. Thai consonants, vowels, tonemarks, symbols) as a string for convenience. There are also few utility functions to test if a string is in Thai or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvoeZg3Igp-",
        "outputId": "baad10c1-0042-4f0c-ef86-c032b29f0909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮฤฦะัาำิีึืุูเแโใไๅํ็่้๊๋ฯฺๆ์ํ๎๏๚๛๐๑๒๓๔๕๖๗๘๙฿'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "pythainlp.thai_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPtK_FL-UAI",
        "outputId": "17ee9c08-cfe9-4e81-9da8-e50a746632b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "len(pythainlp.thai_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPwx53A6IgqF",
        "outputId": "8b182025-6df4-4206-9962-206cd602575b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "pythainlp.thai_consonants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5-lZjsd-UAJ",
        "outputId": "d909bd7b-1fd6-412d-8bfd-5f48e45d7380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(pythainlp.thai_consonants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UA7Hwy_IgqI",
        "outputId": "43bf6a76-c28f-4982-e723-fe97d522f518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32vFPqeB-UAJ"
      },
      "source": [
        "## Checking if a string contains Thai character or not, or how many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3NvXqYFIgqK",
        "outputId": "14b1354a-84d3-488f-98c2-63e19f6c0a80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import pythainlp.util\n",
        "\n",
        "pythainlp.util.isthai(\"ก\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRzSQjugIgqM",
        "outputId": "c5dcaae3-a3b6-4264-d1ed-fe9bf41d05a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP5yfJebIgqP",
        "outputId": "6ac6e804-a869-4ac7-900a-57c4989a8efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\", ignore_chars=\".()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ภาษาระดับกึ่งทางการ\n"
      ],
      "metadata": {
        "id": "FxkLG5Av-BoR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFPOHyZIgqh"
      },
      "source": [
        "# Tokenization and Segmentation\n",
        "\n",
        "At sentence, word, and sub-word levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzDO2rIP-UAN"
      },
      "source": [
        "### Sentence\n",
        "\n",
        "Default sentence tokenizer is \"crfcut\". Tokenization engine can be chosen ussing `engine=` parameter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "from pythainlp.tokenize import subword_tokenize\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.tag import pos_tag"
      ],
      "metadata": {
        "id": "WDZtB1BND8m_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category มหาวิทยาลัย\n",
        "```\n",
        "1.สวัสดีค่ะอาจารย์ที่ปรึกษา\n",
        "2.นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\n",
        "3.ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\n",
        "4.นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\n",
        "5.ผมขอสอบถามเกี่ยวกับการเตรียมพร้อมในการสอบวิชาอาจาร์ยหน่อยครับ\n",
        "6.นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\n",
        "7.นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\n",
        "8.คุณคือหัวหน้า เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\n",
        "9.นักศึกษาหมดกำลังใจในการทำงาน\n",
        "10.อาจาร์ยที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\n",
        "```"
      ],
      "metadata": {
        "id": "PN8cnBkOV-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SklPJ-DbIgqi"
      },
      "source": [
        "### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "print(\"newmm  :\", word_tokenize(text1))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text1, engine=\"longest\"))\n",
        "\n",
        "words1 = [\"อาจารย์\", \"ปรึกษา\"]\n",
        "custom_tokenizer = Tokenizer(words1)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKvEDM3U-YGy",
        "outputId": "bcbe51ce-8782-4f8d-eb37-f506ec27f5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm  : ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "longest: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['สวัสดีค่ะ', 'อาจารย์', 'ที่', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "JEbY-MGCIgqi",
        "outputId": "edac99a0-a786-4d5b-b4fd-c05016df7c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm  : ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "longest: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['นักศึกษาสามารถติดต่อขอคำแนะนำได้กับ', 'อาจารย์', 'ที่', 'ปรึกษา']\n"
          ]
        }
      ],
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text2))\n",
        "print(\"newmm  :\", word_tokenize(text2))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text2, engine=\"longest\"))\n",
        "\n",
        "words2 = [\"อาจารย์\", \"ปรึกษา\"]\n",
        "custom_tokenizer = Tokenizer(words2)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text3))\n",
        "print(\"newmm  :\", word_tokenize(text3))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text3, engine=\"longest\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc9PVhShepRM",
        "outputId": "268d4777-a1a5-4eeb-c90f-2ab3ede0e7c6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "newmm  : ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "longest: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text4))\n",
        "print(\"newmm  :\", word_tokenize(text4))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text4, engine=\"longest\"))\n",
        "\n",
        "words4 = [\"โน็ตบุ๊ค\"]\n",
        "custom_tokenizer = Tokenizer(words4)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7EBXclqYM6k",
        "outputId": "c91fcd63-8fb1-4bb1-c800-614492ced5fd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "newmm  : ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "longest: ['นักศึกษา', 'ต้องการ', 'โน็', 'ตบุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "newmm (custom dictionary): ['นักศึกษาต้องการ', 'โน็ตบุ๊ค', 'ในการทำงานหรือไม่']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAevMAUUYegx"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_detokenize\n",
        "print(word_detokenize(['โน็', 'ตบุ๊ค']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ffd93a-4582-4142-a340-666678fbad63",
        "id": "p7Q3qoEPYegy"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "โน็ตบุ๊ค\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category ที่ทำงาน\n",
        "```\n",
        "11.เคยใช้กูเกิ้ลโครมเพื่อแชร์หรือไฟล์ในกูเกิ้ลไดร์ฟไหม\n",
        "12.ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\n",
        "13.ขอยืมเมาส์บลูทูธและแป้นพิมพ์ของเธอได้มั้ย\n",
        "14.ทักทายเพื่อนที่เป็นสมาชิกใหม่ในองค์กรคมนาคม\n",
        "15.ประสบการณ์การใช้หูฟังไร้สายช่วยลดความเพลียได้\n",
        "16.เห็นด้วยกับระเบียบใหม่ที่ให้อนุมัติได้ง่ายขึ้น\n",
        "17.อย่าดูถูกคนที่มีอายุน้อยและประสบการณ์ที่น้อยกว่า\n",
        "18.หมดแรงเพราะหน้าที่นี้มีการทำงานหนักมาก\n",
        "19.ถ้าคุณแนบเอกสารในกูเกิ้ลไดร์ฟแล้ว ช่วยตอบกลับอีเมลด้วย\n",
        "20.สามารถเขียนเว็บไซต์จากโปรแกรมคอมพิวเตอร์\n",
        "```\n"
      ],
      "metadata": {
        "id": "uQ9hn2tjOXP7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5P_YygrIgqm"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_detokenize\n",
        "print(word_detokenize([\"กูเกิ้ล\", \"ไดร์\", \"ฟ\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJTZ6l1FR3sk",
        "outputId": "728645e1-cee8-4b52-f0ec-c21e72e75f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "กูเกิ้ลไดร์ฟ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI_Qz3k3Igqm",
        "outputId": "00585ca6-3bca-4870-80b0-960e1ad1d86e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newmm  : ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "longest: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "newmm (custom dictionary): ['เคยใช้', 'กูเกิ้ลโครม', 'เพื่อแชร์รูปภาพใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n"
          ]
        }
      ],
      "source": [
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"newmm  :\", word_tokenize(text))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text, engine=\"longest\"))\n",
        "\n",
        "words = [\"กูเกิ้ลโครม\", \"กูเกิ้ลไดร์ฟ\"]\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIXUxXlTIgqo"
      },
      "source": [
        "Default word tokenizer use a word list from `pythainlp.corpus.common.thai_words()`.\n",
        "We can get that list, add/remove words, and create new tokenizer from the modified list."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))"
      ],
      "metadata": {
        "id": "JnPHAqegLPPJ",
        "outputId": "3d22db46-169b-4afb-990e-03c84d11d74e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "custom dictionary : ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop Word Removal\n"
      ],
      "metadata": {
        "id": "Tc9PTjMLXP7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ฉันชอบเรียนวิชาภาษาไทยที่โรงเรียนในตอนเช้า\"\n",
        "\n",
        "# ตัดคำ\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# ดึงรายการ stop words ในภาษาไทย\n",
        "stopwords = thai_stopwords()\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "o9UOjfeQX9o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech tagging"
      ],
      "metadata": {
        "id": "_9FW41E0WuWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ernbPKbDOq2w",
        "outputId": "3f4e6a3f-ba30-49ce-e7db-96f488f52f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('เคย', 'XVMM'),\n",
              " ('ใช้', 'VACT'),\n",
              " ('กูเกิ้ลโครม', 'NCMN'),\n",
              " ('เพื่อ', 'RPRE'),\n",
              " ('แชร์', 'VACT'),\n",
              " ('รูปภาพ', 'NCMN'),\n",
              " ('ใน', 'RPRE'),\n",
              " ('กูเกิ้ลไดร์ฟ', 'NCMN'),\n",
              " ('ไหม', 'NCMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "IdPMtwGPqLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "\n",
        "def reorder_to_tsl(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Basic reordering to follow TSL rules\n",
        "    # Step 1: Move time words to the beginning\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # Join the reordered sentence back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# Example sentence in Thai\n",
        "sentence1 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence to follow TSL grammar rules\n",
        "tsl_sentence = reorder_to_tsl(sentence1)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "id": "Iay6ZOMRm5kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64682669-6e47-4bd3-e136-6f1490ff5203"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reordered sentence (TSL): พรุ่งนี้ ฉัน ไป ตลาด\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Combine and remove duplicates\n",
        "    reordered_sentence = list(dict.fromkeys(time_elements + nouns + others + verbs))\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "1_YvPxmnyLuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b114f3a5-9ad5-4848-ca77-b824ef64c40c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา: พรุ่งนี้ ตลาด ฉัน ไป\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "vG3y24f52B2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "\n",
        "print(get_lemma(\"ที่ปรึกษา\"))"
      ],
      "metadata": {
        "id": "1w-R7DtYCsF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3129d0-578f-4b6e-923a-a9cf0c995a8d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ปรึกษา\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "# ฟังก์ชันการหาคำพื้นฐาน (lemma)\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# ฟังก์ชันการเรียงตามไวยากรณ์ TSL และเรียงคำนามก่อนกริยา พร้อมการใช้ lemma\n",
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Apply the custom get_lemma function to all words in the sentence\n",
        "    sentence_after_lemma = [get_lemma(word) for word in sentence_after_tsl]\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Reorder: Nouns -> Others -> Verbs\n",
        "    reordered_sentence = time_elements + nouns + others + verbs\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "MLZ07UzPEC8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecbdaaa-ba3e-45d8-9c41-291baadb6317"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma: สวัสดี ค่ะ อาจารย์ที่ปรึกษา\n"
          ]
        }
      ]
    }
  ]
}
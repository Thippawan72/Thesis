{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ri5cVDAWIgp7",
        "MqR6Klwc-UAH",
        "A6gy4MLGIgp9",
        "32vFPqeB-UAJ",
        "B9hosQNsPiw0",
        "_9FW41E0WuWg",
        "IdPMtwGPqLDK"
      ],
      "mount_file_id": "1MsIwZL9GB6DnXojABJF5N2V1zIPRxx3d",
      "authorship_tag": "ABX9TyOq8OGDb1N+s9f09lZGVSKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thippawan72/Thesis/blob/main/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VhBw2AmK4sGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "jEVHz8jCFOnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install moviepy"
      ],
      "metadata": {
        "id": "m23WOv5CmOHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f05dnQR43St",
        "outputId": "06e4a4ac-b514-4101-bad5-776b2904caf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "e4GtPYPDnl3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input and output directories\n",
        "formal_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/วิดีโอทั้งหมดกึ่งทางการ'\n",
        "formal_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/Video_กึ่งทางการ'\n",
        "\n",
        "casual_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /วิดีโอทั้งหมดการใช้ชีวิตประจำวัน'\n",
        "casual_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /Video_การใช้ชีวิตประจำวัน'"
      ],
      "metadata": {
        "id": "dxA0Dw77n1ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert GIF to MP4\n",
        "def convert_gif_to_mp4(input_gif, output_mp4):\n",
        "    try:\n",
        "        # Load the GIF file\n",
        "        clip = VideoFileClip(input_gif)\n",
        "\n",
        "        # Write the video to MP4 format\n",
        "        clip.write_videofile(output_mp4, codec='libx264')\n",
        "        print(f\"Successfully converted {input_gif} to {output_mp4}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {input_gif}: {e}\" )"
      ],
      "metadata": {
        "id": "1OSXPthpm8r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all GIFs in a directory\n",
        "#def preprocess_videos(input_dir, output_dir):\n",
        "#   if not os.path.exists(output_dir):\n",
        " #       os.makedirs(output_dir)\n",
        "\n",
        "#    # Loop over all files in the input directory\n",
        " #   for filename in os.listdir(input_dir):\n",
        "  #      if filename.endswith(\".gif\"):\n",
        "   #         input_path = os.path.join(input_dir, filename)\n",
        "    #        output_filename = filename.replace(\".gif\", \".mp4\")\n",
        "     #       output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            # Convert GIF to MP4\n",
        "      #      convert_gif_to_mp4(input_path, output_path)\n",
        "\n",
        "# Start preprocessing the GIFs\n",
        "#preprocess_videos(formal_input_directory, formal_output_directory)"
      ],
      "metadata": {
        "id": "HVpZAsKSna4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5cVDAWIgp7"
      },
      "source": [
        "# PyThaiNLP Get Started\n",
        "\n",
        "Code examples for basic functions in PyThaiNLP https://github.com/PyThaiNLP/pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HsfhZlwInqs"
      },
      "outputs": [],
      "source": [
        "# # pip install required modules\n",
        "# # uncomment if running from colab\n",
        "# # see list of modules in `requirements` and `extras`\n",
        "# # in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
        "\n",
        "#!pip install pythainlp\n",
        "#!pip install epitran"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install required modules"
      ],
      "metadata": {
        "id": "BaT_g8fV-7xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install epitran"
      ],
      "metadata": {
        "id": "E32blbWe_CLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1999414-2679-479c-e0dc-21262e55c30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.8.30)\n",
            "Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.4\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.9.11)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n",
            "Collecting jamo (from epitran)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting g2pk (from epitran)\n",
            "  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n",
            "Collecting konlpy (from g2pk->epitran)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting python-mecab-ko (from g2pk->epitran)\n",
            "  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n",
            "Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n",
            "Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=a5fb21c7b1a1fd7e7e25384ecd5a24a1e0f8abec0a2fd29cba28582cadbf59c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n",
            "Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR6Klwc-UAH"
      },
      "source": [
        "## Import PyThaiNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp"
      ],
      "metadata": {
        "id": "9IhnIXQJ-oDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7CkITTf-UAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975232b7-5622-4b1c-8c3b-190171d2a192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5.0.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6gy4MLGIgp9"
      },
      "source": [
        "## Thai Characters\n",
        "\n",
        "PyThaiNLP provides some ready-to-use Thai character set (e.g. Thai consonants, vowels, tonemarks, symbols) as a string for convenience. There are also few utility functions to test if a string is in Thai or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAvoeZg3Igp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feee4b60-f4eb-470a-b549-c2db2160b137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮฤฦะัาำิีึืุูเแโใไๅํ็่้๊๋ฯฺๆ์ํ๎๏๚๛๐๑๒๓๔๕๖๗๘๙฿'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "pythainlp.thai_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPtK_FL-UAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b0ce0b-e0f1-4c19-e7b0-825f9778e79b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(pythainlp.thai_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPwx53A6IgqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7c65d0-2de9-47e9-b623-ecdacd3bf12e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pythainlp.thai_consonants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5-lZjsd-UAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed6a7a4-91de-4279-d24d-3944b6ffa8f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(pythainlp.thai_consonants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UA7Hwy_IgqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a2b142-8dba-4b47-adf4-56de80bd3870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32vFPqeB-UAJ"
      },
      "source": [
        "## Checking if a string contains Thai character or not, or how many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3NvXqYFIgqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c51a5b-31f6-4180-a966-90b8ed1301e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import pythainlp.util\n",
        "\n",
        "pythainlp.util.isthai(\"ก\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRzSQjugIgqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e13961-73d2-4617-c87c-01a2998c77eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP5yfJebIgqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d50e58-46b5-4be7-f5aa-8426eb57d0bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\", ignore_chars=\".()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ภาษาระดับกึ่งทางการ\n"
      ],
      "metadata": {
        "id": "FxkLG5Av-BoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "from pythainlp.tokenize import subword_tokenize\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus import thai_stopwords"
      ],
      "metadata": {
        "id": "WDZtB1BND8m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category มหาวิทยาลัย\n",
        "```\n",
        "1.สวัสดีค่ะอาจารย์ที่ปรึกษา\n",
        "2.นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\n",
        "3.ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\n",
        "4.นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\n",
        "5.ผมขอสอบถามเกี่ยวกับการเตรียมพร้อมในการสอบวิชาอาจาร์ยหน่อยครับ\n",
        "6.นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\n",
        "7.นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\n",
        "8.คุณคือหัวหน้า เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\n",
        "9.นักศึกษาหมดกำลังใจในการทำงาน\n",
        "10.อาจาร์ยที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\n",
        "```"
      ],
      "metadata": {
        "id": "PN8cnBkOV-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFPOHyZIgqh"
      },
      "source": [
        "###Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5P_YygrIgqm"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIXUxXlTIgqo"
      },
      "source": [
        "Default word tokenizer use a word list from `pythainlp.corpus.common.thai_words()`.\n",
        "We can get that list, add/remove words, and create new tokenizer from the modified list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SklPJ-DbIgqi"
      },
      "source": [
        "### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "words = set(thai_words())\n",
        "words.add(\"อาจารย์\")\n",
        "words.add(\"ปรึกษา\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text1))"
      ],
      "metadata": {
        "id": "IKvEDM3U-YGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ad0df5-fe4c-4825-e539-a87199fc6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEbY-MGCIgqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76dbcaf1-e6a4-42cc-ece2-c94016079634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ],
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text2))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text3))"
      ],
      "metadata": {
        "id": "Dc9PVhShepRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00bcf1c-31b9-47f6-a837-b7501f0bc395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text4))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text4))"
      ],
      "metadata": {
        "id": "D7EBXclqYM6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95974155-ba95-4970-a0d5-6ef4d312fd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "custom dictionary: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text5))\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYqQnvnZ1V7P",
        "outputId": "698f9225-2807-46ed-b342-7a6f16bea6d7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text6))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14EMK8Db1WDU",
        "outputId": "97cc8da4-b431-4b3f-aec9-dd846a83cbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่ง', 'สัปดาห์', 'หน้า']\n",
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text7))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqHgG3eo1i5C",
        "outputId": "71b8610d-5b31-4bf7-a354-8591d60e50f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การเรียน', 'ได้']\n",
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text8))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzV1lNl2RvQw",
        "outputId": "733222f2-292b-4669-8993-b41c2daa5e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text9))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPY9kXuJZGCM",
        "outputId": "274d60ef-6f27-4e8d-fb66-186c3560a581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การทำงาน']\n",
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text10))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8W4aQntZGiM",
        "outputId": "8a47d594-3dde-471e-a7fb-ce3699bb097d"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['อาจารย์ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อ', 'หัว', 'นี้']\n",
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_detokenize\n",
        "print(word_detokenize(['โน็', 'ตบุ๊ค']))"
      ],
      "metadata": {
        "id": "p7Q3qoEPYegy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fa1c4e-1556-4159-9cb6-9efb4852890d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "โน็ตบุ๊ค\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Word Removal\n"
      ],
      "metadata": {
        "id": "lRuhGEg-O53A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ตัดคำ\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text1))\n",
        "words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "# ดึงรายการ stop words ในภาษาไทย\n",
        "stopwords = thai_stopwords()\n",
        "# แปลง stop words เป็นเซ็ต (set) สำหรับการเพิ่ม/ลบ\n",
        "\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_8keLiHlENu",
        "outputId": "514f9fd5-1931-4b8b-ff5c-24473853ac8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text2))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text2))\n",
        "words = custom_tokenizer.word_tokenize(text2)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "FrLPcE7cO53B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c7fbf0-9e41-47a4-c6bb-e2664fc95b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text3))\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text3))\n",
        "words = custom_tokenizer.word_tokenize(text3)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c497kmcx91yE",
        "outputId": "248e286f-6f6f-42c2-f62e-b7e6832bef6e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "custom dictionary : ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Original words: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Filtered words: ['ขอบคุณ', 'อาจารย์', 'คำแนะนำ', 'เกี่ยวกับ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text4))\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text4))\n",
        "words = custom_tokenizer.word_tokenize(text4)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDO0LL50HIbB",
        "outputId": "51b6e099-79b8-4c5c-b380-d35415429357"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "custom dictionary : ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Original words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Filtered words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text5))\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text5))\n",
        "words = custom_tokenizer.word_tokenize(text5)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "0A45r2s7_aI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98da9685-7d5a-46e3-e238-ba65b80034ce"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Original words: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Filtered words: ['ผม', 'สอบถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text6))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text6))\n",
        "words = custom_tokenizer.word_tokenize(text6)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKpgpjKnQJE-",
        "outputId": "0ec99db9-f0da-4206-a6e9-7f93a615bc0e"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่ง', 'สัปดาห์', 'หน้า']\n",
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Original words: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Filtered words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text7))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text7))\n",
        "words = custom_tokenizer.word_tokenize(text7)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKyVCPIIQJhc",
        "outputId": "aa0e1a82-aa99-4952-f1a9-3dd1229288dd"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การเรียน', 'ได้']\n",
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text8))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))\n",
        "words = custom_tokenizer.word_tokenize(text8)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"คุณ\")\n",
        "stopwords.discard(\"เขียน\")\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.add(\" \")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4uthukQJ-g",
        "outputId": "a8b5b20c-8ef7-4678-d380-27a3a1edfc44"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Original words: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Filtered words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text9))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text9))\n",
        "words = custom_tokenizer.word_tokenize(text9)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_20r-hDQZsx",
        "outputId": "806b014f-052b-45cd-f825-ea5a199d9404"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การทำงาน']\n",
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Original words: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Filtered words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text10))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text10))\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))\n",
        "words = custom_tokenizer.word_tokenize(text10)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.discard(\"ตาม\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfu-82g3QaFc",
        "outputId": "ed1c8a54-6d10-4ba1-ba53-d7c26d130ddd"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['อาจารย์ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อ', 'หัว', 'นี้']\n",
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "Original words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Filtered words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech tagging"
      ],
      "metadata": {
        "id": "B9hosQNsPiw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ZNUCg1s3Piw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27513d8f-0086-4ab6-f3d3-f95af1037597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('สวัสดี', 'NCMN'), ('อาจารย์', 'NCMN'), ('ที่ปรึกษา', 'NCMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tsl_grammar(words):\n",
        "    # ตัวอย่างกฎพื้นฐานสำหรับ TSL\n",
        "    words = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
        "    pos_tag(words)\n",
        "\n",
        "    expected_order = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา'] # รูปแบบที่คาดหวัง\n",
        "    tags = pos_tag(words)  # รับ POS tags สำหรับคำ\n",
        "    print(\"POS Tags:\", tags)\n",
        "\n",
        "    # ตรวจสอบการเรียงลำดับพื้นฐานตามกฎที่กำหนดเอง\n",
        "    for i, word in enumerate(words):\n",
        "        if word in ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']:  # คำบอกเวลา\n",
        "            if i != 0:  # คำบอกเวลาควรอยู่ที่ตำแหน่งแรก\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# ใช้ฟังก์ชันเพื่อตรวจสอบ\n",
        "is_correct_order = check_tsl_grammar(words)\n",
        "print(\"Is correct order:\", is_correct_order)"
      ],
      "metadata": {
        "id": "8FqIUNUURcDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5470313a-d1e1-4932-eeb3-b55568b9bcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('สวัสดี', 'NCMN'), ('อาจารย์', 'NCMN'), ('ที่ปรึกษา', 'NCMN')]\n",
            "Is correct order: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "oLM9FnsfPiw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "\n",
        "print(get_lemma(\"ที่ปรึกษา\"))"
      ],
      "metadata": {
        "id": "E-x2DMApPiw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6d0961-e3a4-4c95-81d7-dcb2847e7459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ปรึกษา\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# สร้าง custom dictionary\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "# ใช้ custom tokenizer\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "-SzUZngcPiw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ecd71c-294f-4aee-d462-fc5e7c5c2438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['สวัสดี', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text2)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX0cV3pF867c",
        "outputId": "c5ef0ac7-820b-4204-8be1-217d8b24eefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text3)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYFnDY5h9wGp",
        "outputId": "9558469a-5b8b-47ba-eb51-4b18c80aa21e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Original words: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Filtered words: ['ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Lemmatized words: ['ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text4)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVKekLnHDZt",
        "outputId": "5947dee5-794a-4953-e422-7c854c8f9237"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Original words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Filtered words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n",
            "Lemmatized words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สอบถาม\": \"ถาม\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text5)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzrsaAqeNI4G",
        "outputId": "95213273-55ea-4002-ee5e-0c3c184cdb2a"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Original words: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Filtered words: ['ผม', 'สอบถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n",
            "Lemmatized words: ['ผม', 'ถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text6)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q48HssPL5Uk9",
        "outputId": "d448a584-5b88-400d-9d19-be48535e1897"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Original words: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Filtered words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n",
            "Lemmatized words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text7)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDoCQvM56XS",
        "outputId": "3d6f54c5-2229-4fe3-8430-5b9011c080f1"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n",
            "Lemmatized words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text8)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"คุณ\")\n",
        "stopwords.discard(\"เขียน\")\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.add(\" \")\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_6HG19H56uM",
        "outputId": "76bd9ab6-a655-4a51-90c6-f543a4518999"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Original words: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Filtered words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n",
            "Lemmatized words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text9)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6_BznJO-IdZ",
        "outputId": "f35fe046-1c92-4558-a751-25b0b45c897a"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Original words: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Filtered words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n",
            "Lemmatized words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text10)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.discard(\"ตาม\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzkU_dVH-K2B",
        "outputId": "30d8f6b0-2bf9-408f-ee23-d16860c306cd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Original words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Filtered words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n",
            "Lemmatized words: ['อาจารย์', 'ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "amtvFL9BPiw9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUeRBzOiaIVZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category ที่ทำงาน\n",
        "```\n",
        "11.เคยใช้กูเกิ้ลโครมเพื่อแชร์หรือไฟล์ในกูเกิ้ลไดร์ฟไหม\n",
        "12.ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\n",
        "13.ขอยืมเมาส์บลูทูธและแป้นพิมพ์ของเธอได้มั้ย\n",
        "14.ทักทายเพื่อนที่เป็นสมาชิกใหม่ในองค์กรคมนาคม\n",
        "15.ประสบการณ์การใช้หูฟังไร้สายช่วยลดความเพลียได้\n",
        "16.เห็นด้วยกับระเบียบใหม่ที่ให้อนุมัติได้ง่ายขึ้น\n",
        "17.อย่าดูถูกคนที่มีอายุน้อยและประสบการณ์ที่น้อยกว่า\n",
        "18.หมดแรงเพราะหน้าที่นี้มีการทำงานหนักมาก\n",
        "19.ถ้าคุณแนบเอกสารในกูเกิ้ลไดร์ฟแล้ว ช่วยตอบกลับอีเมลด้วย\n",
        "20.สามารถเขียนเว็บไซต์จากโปรแกรมคอมพิวเตอร์\n",
        "```\n"
      ],
      "metadata": {
        "id": "uQ9hn2tjOXP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization"
      ],
      "metadata": {
        "id": "NTUccqMaOLde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text11 = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text11))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text11))"
      ],
      "metadata": {
        "id": "JnPHAqegLPPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152c6117-dff3-4474-9a69-be233e36bcc1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "custom dictionary : ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text12 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text12))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfqrVQuJaiuN",
        "outputId": "9ed8b178-ea6e-458f-a00c-a5040f5323ca"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text13 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text13))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-s04Rbeb2Qh",
        "outputId": "d501754c-b636-4974-a025-0e81f55dc7b7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text14 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text14))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text14))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov9ngcgAb2pw",
        "outputId": "db210a16-873d-427d-a4f4-cfa2446b6159"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text15 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text15))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItqPSom3b3C1",
        "outputId": "a0bfbea4-c11f-46ee-c011-5032b39ef76c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text16 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text16))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text16))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ZyCtDIb-my",
        "outputId": "bfe594d4-1f20-442f-bded-618fef591e1c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text17 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text17))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text17))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9inXLkkXb_bM",
        "outputId": "20611d07-d29f-471e-ce57-f346a77c9473"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Word Removal\n"
      ],
      "metadata": {
        "id": "Tc9PTjMLXP7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "id": "dny6xk9fzX2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7bed4c-5e33-4e1a-c44a-6c29e6f5d0af"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ใช่ไหม', 'จริงจัง', 'ให้แก่', 'โดย', 'กว้าง', 'ช้า', 'พวก', 'ว่า', 'นอก', 'มุ่งเน้น', 'น่าจะ', 'เมื่อคืน', 'ตลอดทั่วทั้ง', 'จัดหา', 'ได้ที่', 'เป็นเพื่อ', 'ช่วงนี้', 'นํา', 'เสียแล้ว', 'แต่ว่า', 'รวมถึง', 'นู้น', 'ถึงบัดนั้น', 'วัน', 'หนอย', 'พอสม', 'พอแล้ว', 'แล้วกัน', 'ขณะหนึ่ง', 'เสียนั่นเอง', 'นิดๆ', 'เป็นที', 'ใช้', 'หรือไม่', 'มั้ย', 'ก่อนหน้านี้', 'แต่ทว่า', 'นับแต่นั้น', 'เป็นการ', 'กำลังจะ', 'แต่เพียง', 'จวบกับ', 'คุณๆ', 'จนกระทั่ง', 'แก้ไข', 'ทำให้', 'ก่อนๆ', 'นำพา', 'รึว่า', 'เชื่อว่า', 'ช้านาน', 'อันที่จะ', 'เสียนี่', 'ตลอดจน', 'บางแห่ง', 'เมื่อวันวาน', 'เสียยิ่งนัก', 'กัน', 'ทุกคน', 'ขาด', 'ครั้ง', 'ดังกับว่า', 'เป็นอันว่า', 'เกี่ยวกับ', 'จัง', 'ทีละ', 'ดัง', 'แท้', 'อย่างโน้น', 'เชื่อมั่น', 'แรก', 'ไกล', 'เท่าไร', 'แต่เดิม', 'ใต้', 'เท่านั้น', 'นานๆ', 'ภายใน', 'เช่นที่', 'เห็น', 'เยอะ', 'น้อย', 'มั้ยล่ะ', 'ร่วมมือ', 'ส่วนด้อย', 'นำมา', 'นั่นเอง', 'แค่ไหน', 'ข้าพเจ้า', 'ไหนๆ', 'เมื่อวาน', 'หลังจาก', 'หน่อย', 'เช่นเคย', 'เลย', 'ยกให้', 'ดังเก่า', 'เป็นเพราะ', 'ทีไร', 'คราวหลัง', 'เข้า', 'เพิ่มเติม', 'ภายหน้า', 'เป็นอัน', '\\ufeffๆ', 'อย่างไหน', 'เรื่อย', 'ทว่า', 'ซึ่งได้แก่', 'ทั้งสิ้น', 'มั้ยเนี่ย', 'ยอมรับ', 'เฉยๆ', 'จริงๆจังๆ', 'คราที่', 'เกี่ยวๆ', 'ครั้งคราว', 'ถูก', 'ส่ง', 'เพียง', 'แค่จะ', 'ส่วนใด', 'พา', 'บอกแล้ว', 'เกือบจะ', 'ช่วงระหว่าง', 'บัดนั้น', 'ทั้ง', 'เสียยิ่ง', 'อย่างไร', 'ก่อน', 'อื่นๆ', 'พบว่า', 'น่า', 'อะไร', 'กันนะ', 'เพื่อที่', 'ใหม่', 'ตามๆ', 'เป็นต้นมา', 'ส่วนน้อย', 'พวกกู', 'จัดตั้ง', 'เก็บ', 'เผื่อ', 'เดียวกัน', 'ยัง', 'ไกลๆ', 'บอก', 'ร่วม', 'พร้อมด้วย', 'เป็น', 'จำเป็น', 'ระยะๆ', 'นาง', 'ถ้าจะ', 'แต่เมื่อ', 'เราๆ', 'สมัยนั้น', 'ค่ะ', 'ครา', 'ผิดๆ', 'ส่วน', 'ที่', 'รือ', 'ขวาง', 'เหล่า', 'ทั้งๆ', 'กว้างขวาง', 'กันเถอะ', 'แค่เพียง', 'ไว้', 'เพิ่งจะ', 'พวกนี้', 'กล่าวคือ', 'วันไหน', 'เพิ่ม', 'เรียก', 'นั้นไว', 'ซะจน', 'เน้น', 'จู่ๆ', 'ตลอดทั้ง', 'เหตุนี้', 'ร่วมกัน', 'ซะจนกระทั่ง', 'ล้วนแต่', 'สําหรับ', 'ภายใต้', 'อันไหน', 'คราวที่', 'เท่าใด', 'กลับ', 'ได้มา', 'ครั้งใด', 'เต็มไปหมด', 'นับตั้งแต่', 'พวกนู้น', 'อยู่', 'คราวก่อน', 'เช่นที่เคย', 'เช่นดังที่', 'จึงเป็น', 'เช่นที่ว่า', 'ทั้งนั้นเพราะ', 'ทุกที่', 'สิ้น', 'ดังกับ', 'เป็นต้นไป', 'เต็มไปด้วย', 'จด', 'มิใช่', 'สมัยโน้น', 'อย่างมาก', 'ที่แล้ว', 'ครั้งหนึ่ง', 'ดั่งกับว่า', 'ไร', 'แห่ง', 'นี่', 'ดั่งเคย', 'อาจเป็น', 'เป็นเพียง', 'พวกนั้น', 'เพื่อให้', 'กระทั่ง', 'ทาง', 'นี้แหล่', 'แหละ', 'ทุกที', 'เท่ากับ', 'นี่ไง', 'นับแต่', 'ฉะนี้', 'ตลอดวัน', 'ไม่ใช่', 'สิ่งนั้น', 'ณ', 'อาจจะ', 'ทั้งนั้น', 'ทั้งที่', 'บางที', 'จนเมื่อ', 'ด้วยเช่นกัน', 'ใคร่จะ', 'รวมกัน', 'นาน', 'นั่น', 'ปรับ', 'พวกเธอ', 'ไม่ว่า', 'ทุกชิ้น', 'น้อยๆ', 'ทําให้', 'หารือ', 'เพียงแค่', 'พวกมึง', 'ต่อกัน', 'ยืนนาน', 'ตลอดระยะเวลา', 'มันๆ', 'นางสาว', 'คราว', 'เสร็จ', 'เสียก่อน', 'ตามที่', 'ที่จริง', 'จนแม้น', 'กล่าว', 'เช่นใด', 'อยาก', 'เผื่อจะ', 'ซึ่งก็', 'เสมือนว่า', 'รึ', 'จวบจน', 'เห็นจะ', 'ช่วงแรก', 'เช่นนั้น', 'อันที่', 'ระยะ', 'เดียว', 'วันนี้', 'เปิด', 'ตั้ง', 'นอกนั้น', 'ล่าสุด', 'ไม่ค่อยเป็น', 'กันและกัน', 'ทั้งคน', 'เป็นอันๆ', 'เหตุผล', 'ก็', 'ที่ได้', 'จ๋า', 'ครั้งนั้น', 'ขั้น', 'กันดีไหม', 'ดั่งเก่า', 'ฝ่ายใด', 'ค่อย', 'เพราะฉะนั้น', 'จ๊ะ', 'เกี่ยวเนื่อง', 'จะได้', 'มอง', 'ง่ายๆ', 'คราใด', 'เปลี่ยนแปลง', 'นิด', 'จากนี้', 'ตามด้วย', 'นับจากนี้', 'ทีๆ', 'จัดแจง', 'จรดกับ', 'ประการฉะนี้', 'ข้างล่าง', 'ค่อยๆ', 'คล้ายกับ', 'รวด', 'ดั่ง', 'คราวหนึ่ง', 'นั่นแหละ', 'ผิด', 'ไป่', 'คล้าย', 'ยังจะ', 'ยิ่งใหญ่', 'ช่วงต่อไป', 'ครั้งละ', 'แห่งโน้น', 'พวกเขา', 'เพียงแต่', 'นับแต่ที่', 'อันที่จริง', 'นู่น', 'อย่างหนึ่ง', 'มาก', 'ถ้า', 'แต่อย่างใด', 'ด้วยว่า', 'เท่านี้', 'หนอ', 'ถือ', 'ครบ', 'บน', 'จนแม้', 'ตามแต่', 'อย่างไรก็ได้', 'คราวโน้น', 'ทัน', 'นี่นา', 'ยังโง้น', 'ถูกๆ', 'ออก', 'เช่นดังเก่า', 'ครั้งๆ', 'สูงส่ง', 'เช่นดังก่อน', 'มีแต่', 'คราวๆ', 'ฯลฯ', 'หากแม้', 'มั้ยนะ', 'หมดสิ้น', 'เคย', 'จัดให้', 'ความ', 'พอจะ', 'ก็ได้', 'เฉกเช่น', 'เสียจน', 'ใครๆ', 'ล้วน', 'หรือเปล่า', 'เพียงเพราะ', 'น้อยกว่า', 'อย่างเช่น', 'อย่างนี้', 'สมัย', 'แต่ต้อง', 'พอสมควร', 'อนึ่ง', 'ข้า', 'ทุกวัน', 'ตรงๆ', 'แค่ว่า', 'จนถึง', 'ข้างบน', 'ด้วยประการฉะนี้', 'ไหน', 'ข้าฯ', 'เหตุนั้น', 'อย่างไรก็', 'เสร็จกัน', 'บางคราว', 'ฯ', 'ต่างๆ', 'ไม่เป็นไร', 'กับ', 'บ่อยๆ', 'ยืนยัน', 'กันไหม', 'ยิ่งขึ้นไป', 'สบาย', 'อาจเป็นด้วย', 'ทั้งหมด', 'หรือไร', 'อื่น', 'อันใด', 'ตลอดกาลนาน', 'แม้กระทั่ง', 'ครัน', 'สู่', 'ส่วนนั้น', 'ทุกหน', 'เขียน', 'จวน', 'ชาว', 'มากมาย', 'อย่างๆ', 'อาจ', 'จวนจะ', 'เป็นๆ', 'เห็นว่า', 'แสดงว่า', 'แห่งไหน', 'ช่วงถัดไป', 'สูงกว่า', 'อย่างใด', 'เมื่อไหร่', 'เช่นนั้นเอง', 'ไฉน', 'ประสบ', 'เปิดเผย', 'และ', 'มักจะ', 'คิดว่า', 'ดังเคย', 'เช่นเดียวกัน', 'ช่วงท้าย', 'เธอ', 'รวมๆ', 'คราวละ', 'ด้วยเหตุว่า', 'ทั้งมวล', 'พวกที่', 'ซะก่อน', 'ในช่วง', 'เป็นที่สุด', 'มองว่า', 'เหตุ', 'ได้แก่', 'แค่', 'ประกอบ', 'เพื่อที่จะ', 'รวมทั้ง', 'ก็ตามแต่', 'คราวหน้า', 'ประการ', 'แค่นั้น', 'จ้า', 'เกือบๆ', 'หากว่า', 'เนี่ยเอง', 'นี่เอง', 'ทํา', 'พร้อมทั้ง', 'เข้าใจ', 'ด้าน', 'พื้นๆ', 'พอที่', 'ยังแต่', 'นอกเหนือ', 'จ้ะ', 'มัน', 'ที่ใด', 'ราย', 'ต่างหาก', 'อย่างน้อย', 'พอๆ', 'วันนั้น', 'แก', 'กระทำ', 'ยาวนาน', 'บางกว่า', 'เช่นเมื่อ', 'บ้าง', 'บ่อยกว่า', 'ของ', 'เฉพาะ', 'เอ็ง', 'เท่าไหร่', 'ตลอดปี', 'มิฉะนั้น', 'มัก', 'แต่ก็', 'เป็นอาทิ', 'จริงๆ', 'จากนี้ไป', 'เมื่อเช้า', 'ลง', 'แห่งใด', 'ถึงเมื่อใด', 'บาง', 'พร้อมเพียง', 'ตลอดกาล', 'ถึงเมื่อ', 'แห่งนี้', 'ยิ่งจะ', 'เมื่อนี้', 'ให้แด่', 'ตลอดไป', 'พวกคุณ', 'เร็วๆ', 'เล็กๆ', 'ในระหว่าง', 'ถึงจะ', 'ให้ดี', 'คราไหน', 'นั้น', 'จวนเจียน', 'ขึ้น', 'ทุกวันนี้', 'ไง', 'คราวไหน', 'จะ', 'เรียบ', 'เนื่องจาก', 'ที่แท้จริง', 'คราวนี้', 'ก็ตาม', 'ทันใดนั้น', 'ถึงแม้จะ', 'ถึงบัดนี้', 'คงอยู่', 'คิด', 'ปิด', 'ทรง', 'จนขณะนี้', 'นอกจาก', 'ประการหนึ่ง', 'ภายภาคหน้า', 'เพิ่ง', 'ร่วมด้วย', 'ครบครัน', 'พึ่ง', 'หลัง', 'เมื่อ', 'จนบัดนี้', 'ยิ่งเมื่อ', 'ที่ซึ่ง', 'ทั้งปวง', 'ส่วนมาก', 'ซึ่งๆ', 'สิ่งใด', 'ด้วยกัน', 'คำ', 'ทั้งตัว', 'นี่แน่ะ', 'ครบถ้วน', 'เช่นนี้', 'มั้ยนั่น', 'ถ้าหาก', 'ผ่านๆ', 'ใน', 'พวกโน้น', 'ทั้งนั้นด้วย', 'เหลือ', 'เมื่อคราวที่', 'นะ', 'หมด', 'สิ่งนี้', 'ทันทีทันใด', 'ยิ่งนัก', 'ข้างเคียง', 'เพียงไร', 'ทุกครั้ง', 'ก็จะ', 'ครั้งนี้', 'จริง', 'สูงๆ', 'ก็ต่อเมื่อ', 'กำหนด', 'เป็นเพียงว่า', 'นอกจากที่', 'เพียงพอ', 'ครานี้', 'ทั่ว', 'ครั้งหลัง', 'ซึ่ง', 'ทั้งเป็น', 'แต่นั้น', 'แต่ละ', 'ย่อม', 'เมื่อเย็น', 'รือว่า', 'ส่วนที่', 'ผู้', 'เฉย', 'ให้มา', 'ตนเอง', 'รวม', 'ที่ไหน', 'ล้วนจน', 'พึง', 'คราหนึ่ง', 'ด้วยเหตุนี้', 'อย่างยิ่ง', 'พอควร', 'ก็ตามที', 'อย่าง', 'สำคัญ', 'แก่', 'ช่วงที่', 'ทุกคราว', 'ตลอดศก', 'เห็นควร', 'ขณะเดียวกัน', 'เกี่ยวข้อง', 'หาใช่', 'ครั้งหลังสุด', 'นับแต่นี้', 'ช่วงหน้า', 'พยายาม', 'เพื่อว่า', 'เห็นแก่', 'ในที่', 'แม้นว่า', 'ต้อง', 'ทุกทาง', 'ครั้งกระนั้น', 'ก่อนหน้า', 'เนี่ย', 'ที่แห่งนั้น', 'เช่น', 'ทุก', 'เกิน', 'เมื่อคราว', 'ข้าง', 'เสียนี่กระไร', 'หน', 'บางที่', 'นิดหน่อย', 'เถอะ', 'ที่นั้น', 'พวกท่าน', 'ได้รับ', 'ช่วงๆ', 'ยก', 'นำ', 'นี้เอง', 'ตลอดทั่ว', 'บางๆ', 'แต่ก่อน', 'เล็ก', 'หนึ่ง', 'เป็นอันมาก', 'นอกจากนั้น', 'เหลือเกิน', 'อันๆ', 'ขณะนี้', 'ตรง', 'บางครา', 'ค่อยไปทาง', 'ทีเถอะ', 'ภายหลัง', 'รวดเร็ว', 'พร้อมกัน', 'คล้ายกับว่า', 'พอที', 'ง่าย', 'เปลี่ยน', 'นอกจากนี้', 'ด้วยที่', 'ค่อนข้างจะ', 'นั่นเป็น', 'เสมือนกับ', 'เป็นแต่เพียง', 'ถูกต้อง', 'ช่วงก่อน', 'เช่นดัง', 'ซะจนถึง', 'นอกเหนือจาก', 'พูด', 'คล้ายกัน', 'นี้', 'ภาค', 'บ่อย', 'บัดเดี๋ยวนี้', 'หาก', 'ยาก', 'ปรากฏว่า', 'ซึ่งกัน', 'แยะ', 'ควร', 'สุด', 'ช่วงหลัง', 'ซึ่งกันและกัน', 'กว่า', 'พร้อม', 'มิ', 'เถิด', 'เมื่อครั้งก่อน', 'ส่วนดี', 'แค่นี้', 'เกี่ยวกัน', 'นาย', 'สูง', 'จนกว่า', 'ตลอดมา', 'นักๆ', 'เพียงใด', 'พร้อมที่', 'จำพวก', 'หากแม้น', 'อย่างดี', 'พอเหมาะ', 'ต่อ', 'เล่าว่า', 'ทีเดียว', 'อันจะ', 'เอง', 'เรื่อยๆ', 'คะ', 'ก็แค่', 'ค่อนมาทาง', 'หากแม้นว่า', 'ขวางๆ', 'ใกล้', 'สูงสุด', 'เป็นต้น', 'ครั้งก่อน', 'ด้วยเพราะ', 'แยะๆ', 'แม้ว่า', 'คือ', 'ต่าง', 'แล้วเสร็จ', 'อดีต', 'กลุ่ม', 'พวกแก', 'ทุกสิ่ง', 'ทุกเมื่อ', 'เช่นไร', 'เต็มๆ', 'ยืนยาว', 'สิ่งไหน', 'มากกว่า', 'คล้ายกันกับ', 'เผื่อว่า', 'เช่นเดียวกับ', 'ในเมื่อ', 'ด้วยเหมือนกัน', 'รวมด้วย', 'เป็นเพราะว่า', 'ภายภาค', 'บางครั้ง', 'เอา', 'อีก', 'ย่อย', 'ผู้ใด', 'ได้', 'เกินๆ', 'ไป', 'มุ่ง', 'เสีย', 'เสียจนถึง', 'เท่าที่', 'ขณะ', 'ภาย', 'ช่วงนั้น', 'ที่ว่า', 'ทุกอย่าง', 'บอกว่า', 'สั้นๆ', 'มุ่งหมาย', 'เล็กน้อย', 'เสียนั่น', 'ก็คือ', 'กันเอง', 'ด้วยเหตุที่', 'ฯล', 'ยาว', 'ผล', 'ผ่าน', 'มา', 'ยิ่งแล้ว', 'จาก', 'ขอ', 'ๆ', 'ทั้งนี้', 'วันใด', 'ที', 'จัด', 'คราวใด', 'ถึงแก่', 'ยิ่งขึ้น', 'หลาย', 'ซึ่งก็คือ', 'ถึง', 'สั้น', 'ใหญ่โต', 'เกิด', 'เกือบ', 'คล้ายว่า', 'เช่นดังว่า', 'แบบ', 'กว้างๆ', 'บัดดล', 'ใดๆ', 'ละ', 'ขณะใดๆ', 'ทั้งหลาย', 'ภาคฯ', 'กระผม', 'ยิ่งจน', 'จับ', 'เสร็จสมบูรณ์', 'แล้ว', 'ถึงแม้ว่า', 'ดังกล่าว', 'ส่วนใหญ่', 'นับจากนั้น', 'นั่นไง', 'ไม่ค่อยจะ', 'เสร็จแล้ว', 'แต่จะ', 'ทุกๆ', 'จัดการ', 'ถึงอย่างไร', 'ใช่', 'เมื่อไร', 'อันละ', 'ด้วย', 'มิได้', 'ด้วยเหตุนั้น', 'ให้', 'ใคร่', 'ประมาณ', 'เชื่อถือ', 'เพื่อ', 'กู', 'ช้าๆ', 'แท้จริง', 'สมัยนี้', 'ฉะนั้น', 'เขา', 'เมื่อครั้ง', 'อันเนื่องมาจาก', 'เป็นดัง', 'เร็ว', 'เริ่ม', 'เมื่อก่อน', 'เพราะว่า', 'บางขณะ', 'บัดนี้', 'อย่างเดียว', 'พอเพียง', 'แห่งนั้น', 'จำ', 'เป็นแต่', 'ตลอดเวลา', 'เช่นกัน', 'สุดๆ', 'กำลัง', 'ขณะที่', 'ฝ่าย', 'ทุกครา', 'ค่อนข้าง', 'ไม่ค่อย', 'เผื่อที่', 'เมื่อนั้น', 'มึง', 'เพราะ', 'อัน', 'ด้วยเหตุเพราะ', 'ถึงเมื่อไร', 'ช่วง', 'แม้แต่', 'มี', 'เท่ากัน', 'เชื่อ', 'พร้อมกับ', 'ตาม', 'สามารถ', 'พวกมัน', 'จึงจะ', 'ยังงี้', 'ฉัน', 'ภายนอก', 'เป็นที่', 'ตั้งแต่', 'ข้างๆ', 'ทุกตัว', 'ทั้งที', 'ได้แต่', 'พอ', 'ทันที', 'เยอะๆ', 'จัดงาน', 'ทุกอัน', 'เคยๆ', 'เมื่อคราวก่อน', 'ระหว่าง', 'ก็แล้วแต่', 'นั้นๆ', 'อย่างนั้น', 'ขณะนั้น', 'ทำไม', 'นัก', 'ที่ละ', 'จนทั่ว', 'แต่ที่', 'พวกฉัน', 'มั๊ย', 'ยังงั้น', 'ที่นี้', 'ยอม', 'นับ', 'ประการใด', 'คง', 'ครั้งที่', 'พบ', 'เสียจนกระทั่ง', 'สืบเนื่อง', 'หรือยัง', 'เพียงเพื่อ', 'อย่างละ', 'กันดีกว่า', 'ใกล้ๆ', 'ที่สุด', 'พอตัว', 'แสดง', 'ยังคง', 'แต่', 'จวบ', 'ใคร', 'พอกัน', 'นี่แหละ', 'พอดี', 'ถือว่า', 'ครั้งครา', 'ครับ', 'น่ะ', 'ดั่งกับ', 'ที่แท้', 'ใหญ่ๆ', 'โตๆ', 'เท่า', 'จนตลอด', 'นอกจากว่า', 'ยิ่ง', 'บ่อยครั้ง', 'ทีใด', 'หรือ', 'เสียด้วย', 'ใหม่ๆ', 'อย่างที่', 'จรด', 'จัดทำ', 'เช่นก่อน', 'แต่ไร', 'เหตุไร', 'ยังไง', 'อย่างไรเสีย', 'ครานั้น', 'ใหญ่', 'เมื่อใด', 'สิ่ง', 'ต่างก็', 'ทำไร', 'ถึงแม้', 'ปฏิบัติ', 'จังๆ', 'ตน', 'ซะ', 'คงจะ', 'รับ', 'ตลอด', 'ขณะใด', 'จึง', 'คราวนั้น', 'สิ้นกาลนาน', 'หรือไง', 'กลุ่มๆ', 'จง', 'ตนฯ', 'ส่วนเกิน', 'เรา', 'ก็ดี', 'เหล่านั้น', 'หมดกัน', 'กระนั้น', 'จน', 'การ', 'ข้างต้น', 'จากนั้น', 'โต', 'สมัยก่อน', 'ทุกแห่ง', 'แต่ไหน', 'พวกกัน', 'ไม่', 'ช่วย', 'แม้', 'อันได้แก่', 'ที่ๆ', 'ปรากฏ', 'หาความ', 'ค่อน', 'เพียงไหน', 'ปัจจุบัน', 'เยอะแยะ', 'คุณ', 'ตลอดถึง', 'เหล่านี้', 'ยืนยง', 'เสร็จสิ้น', 'ครั้งไหน', 'รับรอง', 'ตลอดทั่วถึง', 'กลุ่มก้อน', 'ทำๆ', 'ให้ไป', 'แล้วแต่', 'ยิ่งกว่า', 'เป็นด้วย', 'แต่ถ้า'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "# ตัดคำ\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))\n",
        "words = custom_tokenizer.word_tokenize(text)\n",
        "\n",
        "# ดึงรายการ stop words ในภาษาไทย\n",
        "stopwords = thai_stopwords()\n",
        "# แปลง stop words เป็นเซ็ต (set) สำหรับการเพิ่ม/ลบ\n",
        "\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "# เพิ่มคำใหม่เข้าไปใน stop words\n",
        "stopwords.add(\"ไหม\")\n",
        "stopwords.add(\"ทำงาน\")\n",
        "\n",
        "# ลบคำบางคำออกจาก stop words\n",
        "stopwords.discard(\"เคย\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "o9UOjfeQX9o2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d9608a-30c0-4c21-aba5-3a68ff90a42b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "custom dictionary : ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Original words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Filtered words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'แชร์', 'รูปภาพ', 'กูเกิ้ลไดร์ฟ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech tagging"
      ],
      "metadata": {
        "id": "_9FW41E0WuWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ernbPKbDOq2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a2af4b-4933-4db3-95b3-4434df0696c7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('เคย', 'XVMM'),\n",
              " ('ใช้', 'VACT'),\n",
              " ('กูเกิ้ลโครม', 'NCMN'),\n",
              " ('เพื่อ', 'RPRE'),\n",
              " ('แชร์', 'VACT'),\n",
              " ('รูปภาพ', 'NCMN'),\n",
              " ('ใน', 'RPRE'),\n",
              " ('กูเกิ้ลไดร์ฟ', 'NCMN'),\n",
              " ('ไหม', 'NCMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "vG3y24f52B2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "IdPMtwGPqLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl(words):\n",
        "    # ลบคำที่ไม่จำเป็น เช่น คำบอกเพศ คำเสริม\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # เรียงลำดับโดยย้ายคำบอกเวลาไปด้านหน้า\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # รายการคำบอกเวลา\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # เรียงลำดับ (Time-Topic-Comment)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # รวมคำกลับเป็นประโยค\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ประโยคตัวอย่าง\n",
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "# เพิ่มคำพิเศษใน custom dictionary\n",
        "words = set(thai_words())  # thai_words() คืนค่า frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "\n",
        "# ตัดคำด้วย custom dictionary\n",
        "words_tokenized = custom_tokenizer.word_tokenize(text)\n",
        "\n",
        "# ดึงรายการ stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "\n",
        "# เพิ่มคำใหม่เข้าไปใน stop words\n",
        "stopwords.add(\"ไหม\")\n",
        "stopwords.add(\"ทำงาน\")\n",
        "\n",
        "# ลบคำบางคำออกจาก stop words\n",
        "stopwords.discard(\"เคย\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words_tokenized if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words_tokenized)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้คำที่กรองแล้วมาเรียงลำดับตามกฎ TSL\n",
        "tsl_sentence = reorder_to_tsl(filtered_words)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "id": "qoAZxOzaEPXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50500e3-2de2-498c-a15d-0347e5e95b63"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Filtered words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'แชร์', 'รูปภาพ', 'กูเกิ้ลไดร์ฟ']\n",
            "Reordered sentence (TSL): เคย ใช้ กูเกิ้ลโครม แชร์ รูปภาพ กูเกิ้ลไดร์ฟ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Combine and remove duplicates\n",
        "    reordered_sentence = list(dict.fromkeys(time_elements + nouns + others + verbs))\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "1_YvPxmnyLuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a982178a-c2ca-4725-ed95-af2f500510a7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา: พรุ่งนี้ ตลาด ฉัน ไป\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "\n",
        "print(get_lemma(\"ที่ปรึกษา\"))"
      ],
      "metadata": {
        "id": "1w-R7DtYCsF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094646f4-f2fe-4dd5-83e8-519d892d3d77"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ปรึกษา\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# สร้าง custom dictionary\n",
        "words = set(thai_words())\n",
        "words.add(\"อาจารย์\")\n",
        "words.add(\"ปรึกษา\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.add(\"สวัสดีค่ะ\")  # เพิ่มคำ \"สวัสดีค่ะ\"\n",
        "\n",
        "# ใช้ custom tokenizer\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in tokenized_words]\n",
        "\n",
        "print(\"newmm (custom dictionary):\", tokenized_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "GwDSJ7YXKKif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c74904-8d2f-4787-ac3a-55f21578185e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['สวัสดีค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['สวัสดี', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "# ฟังก์ชันการหาคำพื้นฐาน (lemma)\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# ฟังก์ชันการเรียงตามไวยากรณ์ TSL และเรียงคำนามก่อนกริยา พร้อมการใช้ lemma\n",
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Apply the custom get_lemma function to all words in the sentence\n",
        "    sentence_after_lemma = [get_lemma(word) for word in sentence_after_tsl]\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Reorder: Nouns -> Others -> Verbs\n",
        "    reordered_sentence = time_elements + nouns + others + verbs\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "MLZ07UzPEC8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056c41f5-6d82-4ec2-ac67-5b6987655fef"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma: พรุ่งนี้ ตลาด พรุ่งนี้ ฉัน ไป\n"
          ]
        }
      ]
    }
  ]
}